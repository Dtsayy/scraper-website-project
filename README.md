
# ğŸ•·ï¸ Scraper Website Project  
**Distributed Web Crawler using Scrapy + Redis + PostgreSQL**

---

## ğŸ“˜ Overview

This project â€” **`scraper-project`** â€” is a scalable, distributed web scraping system built with **Scrapy**, **Redis**, and **PostgreSQL**.  
It is designed for **massive data crawling**, **centralized control**, and **parallel execution** through multiple worker nodes.

> ğŸ’¡ The architecture supports multi-domain crawling, HTML storage, and metadata tracking.

---

## âš™ï¸ Tech Stack

| Layer | Technology | Purpose |
| :---- | :---------- | :------- |
| **Crawler Framework** | [Scrapy](https://scrapy.org) | Core crawling engine |
| **Distributed Queue** | [Redis](https://redis.io) | Centralized task queue for worker nodes |
| **Data Storage** | [PostgreSQL](https://www.postgresql.org/) | Metadata & structured data storage |
| **HTML Parser** | [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/) | Cleaning and parsing HTML |
| **Environment Config** | [python-dotenv](https://pypi.org/project/python-dotenv/) | Manage environment variables |
| **Proxy & Rotation** | [scrapy-rotating-proxies](https://pypi.org/project/scrapy-rotating-proxies/), [fake-useragent](https://pypi.org/project/fake-useragent/) | Avoid IP bans |
| **Async Support** | [Twisted](https://twistedmatrix.com/) | Asynchronous networking |
| **HTML Storage** | Network Share / Local Folders | Store prettified HTML content |
| **Task CLI** | Custom Python Scripts | Add URLs, load data, parse results |

---

## ğŸ§© Project Structure

```bash
vendor_scraper/
â”‚
â”œâ”€â”€ vendor_scraper/
â”‚   â”œâ”€â”€ configs/                         # Input configurations
â”‚   â”‚   â”œâ”€â”€ DOM_site.json                # Selector mapping per website
â”‚   â”‚   â””â”€â”€ list_user_agents.txt         # Optional list of user agents
â”‚   â”‚
â”‚   â”œâ”€â”€ dataflow/
â”‚   â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”‚   â”œâ”€â”€ add_url_to_pool.py       # Add URLs to Redis (start_urls)
â”‚   â”‚   â”‚   â””â”€â”€ load_metadata_to_db.py   # Write metadata from Redis to PostgreSQL
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ parse/
â”‚   â”‚   â”‚   â””â”€â”€ parse_html_crawl.ipynb   # Debug and verify HTML parsing
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ process/
â”‚   â”‚       â””â”€â”€ download_img.py          # Download product images
â”‚   â”‚
â”‚   â”œâ”€â”€ middlewares/
â”‚   â”‚   â”œâ”€â”€ base.py                      # Default Scrapy middleware
â”‚   â”‚   â”œâ”€â”€ browser_headers_middleware.py # Fake browser headers (ScrapeOps)
â”‚   â”‚   â”œâ”€â”€ proxy_middleware.py          # Proxy rotation handler
â”‚   â”‚   â””â”€â”€ user_agent_middleware.py     # Fake User-Agent rotation
â”‚   â”‚
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ distributed-worker.py        # Distributed spider using Scrapy-Redis
â”‚   â”‚   â””â”€â”€ playwright_worker.py         # Spider using Playwright (for JS pages)
â”‚   â”‚
â”‚   â”œâ”€â”€ items.py                         # Define item fields for pipeline
â”‚   â”œâ”€â”€ pipelines.py                     # Save HTML & metadata to storage/Redis
â”‚   â””â”€â”€ settings.py                      # Core Scrapy configuration
â”‚
â”œâ”€â”€ pyproject.toml                       # Project and dependency config
â”œâ”€â”€ scrapy.cfg                           # Scrapy entry point
â””â”€â”€ README.md                            # Project documentation
```

---

## âš™ï¸ Features

| Feature                | Description                                         |
| ---------------------- | --------------------------------------------------- |
| **Scrapy + Redis**     | Enables horizontal scaling with multiple workers    |
| **Proxy Rotation**     | Integrated proxy middleware with authentication     |
| **Fake User-Agent**    | Random UA from file or ScrapeOps API                |
| **Playwright Support** | Crawl JavaScript-rendered websites                  |
| **HTML Storage**       | Store cleaned HTML locally or on shared network     |
| **Metadata Queue**     | Push crawl metadata into Redis for later processing |
| **PostgreSQL Loader**  | Auto-insert metadata from Redis into database       |

---

## ğŸš€ How It Works

### 1ï¸âƒ£ Architecture Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       URL Producer       â”‚   â† Push URLs to Redis queue
â”‚  (add_url_to_pool.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Queue (url_pools) â”‚   â† Acts as distributed scheduler
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Distributed Workers    â”‚   â† Multiple Scrapy spiders pulling URLs
â”‚  (spiders/distributed...)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Pipelines         â”‚
â”‚  - Clean HTML            â”‚
â”‚  - Save to storage       â”‚
â”‚  - Push metadata to Redisâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Database / Data Lake   â”‚   â† Store metadata or final data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Core Components

### ğŸ•¸ `distributed-worker.py`

* Worker spider using `RedisSpider` to consume URLs from `url_pools:start_urls`.
* Loads domain-specific rules from `configs/DOM_site.json`.
* Extracts and cleans HTML using CSS selectors.
* Passes output to pipelines for saving.

---

### ğŸ“¦ `items.py`

Defines `ProductItem` model with fields:

```python
domain, url_item, status_code, source_page_html
```

Optionally prettified with **BeautifulSoup** for clean formatting.

---

### âš™ï¸ `pipelines.py`

Handles post-crawl actions:

* Save HTML to storage path.
* Push metadata (URL, domain, file path, status, timestamp) to Redis queue `scrapy:metadata`.
* Organize storage by domain.
* Use SHA256 for unique file naming.

---

### âš™ï¸ `settings.py`

Core Scrapy configuration:

* Enables distributed scheduler (`scrapy_redis.scheduler`).
* Custom downloader middlewares for proxy and UA rotation.
* Controls concurrency, delays, and pipeline priorities.

---

### ğŸ”§ `dataflow/load/add_url_to_pool.py`

Utility script to add new crawl URLs into Redis queue:

```bash
python -m vendor_scraper.dataflow.load.add_url_to_pool "https://example.com/page"
```

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/Dtsayy/scraper-website-project.git
cd cls-scraper-project
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -e .
# or, for dev mode
pip install -e .[dev]
```

### 3ï¸âƒ£ Configure Environment

Create a `.env` file in the root directory:

```env
REDIS_URL=redis://localhost:6379
POSTGRES_URL=postgresql://user:password@localhost:5432/vendor_db
```

### 4ï¸âƒ£ Verify Installation

```bash
scrapy list
# â†’ should show: distributed-worker
```

---

## ğŸ”„ Usage

### â¤ Add URLs to Queue

```bash
python -m vendor_scraper.dataflow.load.add_url_to_pool "https://example.com"
```

### â¤ Start a Distributed Worker

Each worker pulls from Redis automatically:

```bash
scrapy runspider vendor_scraper/spiders/distributed-worker.py
```

### â¤ Monitor Queue

```bash
redis-cli llen url_pools:start_urls
redis-cli llen scrapy:metadata
```

---

## ğŸ§¹ Output Example

**Stored HTML Path**

```text
html_storage/example.com/91e0b1...8f1b.html
```

**Metadata (Redis)**

```json
{
  "url": "https://example.com/page1",
  "domain": "example.com",
  "file_path": "./html_storage/example.com/91e0b1.html",
  "http_status": 200,
  "saved_date": "2025-10-25 12:10:33"
}
```

---

## ğŸ§© Future Improvements

* [ ] Add PostgreSQL loader pipeline
* [ ] Add retry + proxy rotation middleware
* [ ] Build dashboard for Redis queue monitoring
* [ ] Integrate Prefect / Airflow orchestration
* [ ] Automatic sitemap crawling

---

## ğŸ‘¨â€ğŸ’» Authors

**ThanhCD & TienTTM**
*Data Technician | Data Engineer*

---
