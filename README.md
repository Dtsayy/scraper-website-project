
````markdown
# ğŸ•·ï¸ Scraper Website Project  
**Distributed Web Crawler using Scrapy + Redis + PostgreSQL**

---

## ğŸ“˜ Overview

This project â€” **`scraper-project`** â€” is a scalable, distributed web scraping system built with **Scrapy**, **Redis**, and **PostgreSQL**.  
It is designed for **massive data crawling**, **centralized control**, and **parallel execution** through multiple worker nodes.

> ğŸ’¡ The architecture supports multi-domain crawling, HTML storage, and metadata tracking.

---

## âš™ï¸ Tech Stack

| Layer | Technology | Purpose |
|--------|-------------|----------|
| **Crawler Framework** | [Scrapy](https://scrapy.org) | Core crawling engine |
| **Distributed Queue** | [Redis](https://redis.io) | Centralized task queue for worker nodes |
| **Data Storage** | [PostgreSQL](https://www.postgresql.org/) | Metadata & structured data storage |
| **HTML Parser** | [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/) | Cleaning and parsing HTML |
| **Environment Config** | [python-dotenv](https://pypi.org/project/python-dotenv/) | Manage environment variables |
| **Proxy & Rotation** | [scrapy-rotating-proxies](https://pypi.org/project/scrapy-rotating-proxies/), [fake-useragent](https://pypi.org/project/fake-useragent/) | Avoid IP bans |
| **Async Support** | [Twisted](https://twistedmatrix.com/) | Asynchronous networking |
| **HTML Storage** | Network Share / Local Folders | Store prettified HTML content |
| **Task CLI** | Custom Python Scripts | Add URLs, load data, parse results |

---

## ğŸ§© Project Structure

```bash
vendor_scraper/
â”‚
â”œâ”€â”€ vendor_scraper/
â”‚   â”œâ”€â”€ configs/                         # Cáº¥u hÃ¬nh Ä‘áº§u vÃ o
â”‚   â”‚   â”œâ”€â”€ DOM_site.json                # Selector mapping cho tá»«ng website
â”‚   â”‚   â””â”€â”€ list_user_agents.txt         # Danh sÃ¡ch user-agents thá»§ cÃ´ng (tÃ¹y chá»n)
â”‚   â”‚
â”‚   â”œâ”€â”€ dataflow/                        # CÃ¡c module thá»±c thi theo luá»“ng dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”‚   â”œâ”€â”€ add_url_to_pool.py       # ThÃªm URL vÃ o Redis (start_urls)
â”‚   â”‚   â”‚   â””â”€â”€ load_metadata_to_db.py   # Ghi metadata tá»« Redis vÃ o PostgreSQL
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ parse/
â”‚   â”‚   â”‚   â””â”€â”€ parse_html_crawl.ipynb   # Notebook dÃ¹ng Ä‘á»ƒ kiá»ƒm tra HTML & debug
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ process/
â”‚   â”‚       â””â”€â”€ download_img.py          # Táº£i hÃ¬nh áº£nh sáº£n pháº©m tá»« HTML
â”‚   â”‚
â”‚   â”œâ”€â”€ middlewares/                     # Middleware chia theo chá»©c nÄƒng
â”‚   â”‚   â”œâ”€â”€ base.py                      # Middleware máº·c Ä‘á»‹nh (Scrapy template)
â”‚   â”‚   â”œâ”€â”€ browser_headers_middleware.py # Fake browser headers (ScrapeOps)
â”‚   â”‚   â”œâ”€â”€ proxy_middleware.py          # Proxy rotation handler
â”‚   â”‚   â””â”€â”€ user_agent_middleware.py     # Fake User-Agent rotation
â”‚   â”‚
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ distributed-worker.py        # Spider phÃ¢n tÃ¡n dÃ¹ng Redis (Scrapy-Redis)
â”‚   â”‚   â””â”€â”€ playwright_worker.py         # Spider dÃ¹ng Playwright (cho trang Ä‘á»™ng)
â”‚   â”‚
â”‚   â”œâ”€â”€ items.py                         # Äá»‹nh nghÄ©a Item/Field cho pipeline
â”‚   â”œâ”€â”€ pipelines.py                     # Pipeline xá»­ lÃ½, lÆ°u HTML + metadata
â”‚   â””â”€â”€ settings.py                      # Cáº¥u hÃ¬nh Scrapy project
â”‚
â”œâ”€â”€ pyproject.toml                       # Cáº¥u hÃ¬nh project vÃ  dependencies
â”œâ”€â”€ scrapy.cfg                           # Entry point Scrapy
â””â”€â”€ README.md                            # TÃ i liá»‡u dá»± Ã¡n
````

---

## âš™ï¸ Features

| Feature                | Description                                         |
| ---------------------- | --------------------------------------------------- |
| **Scrapy + Redis**     | Enables horizontal scaling with multiple workers    |
| **Proxy Rotation**     | Integrated proxy middleware with authentication     |
| **Fake User-Agent**    | Random UA from file or ScrapeOps API                |
| **Playwright Support** | Crawl JavaScript-rendered websites                  |
| **HTML Storage**       | Store cleaned HTML locally or on shared network     |
| **Metadata Queue**     | Push crawl metadata into Redis for later processing |
| **PostgreSQL Loader**  | Auto-insert metadata from Redis into database       |

---

## ğŸš€ How It Works

### 1ï¸âƒ£ Architecture Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       URL Producer       â”‚  â† Push URLs to Redis queue
â”‚  (add_url_to_pool.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Queue (url_pools) â”‚  â† Acts as distributed scheduler
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Distributed Workers     â”‚  â† Multiple Scrapy spiders pulling URLs
â”‚  (spiders/distributed...)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipelines               â”‚
â”‚  - Clean HTML            â”‚
â”‚  - Save to storage       â”‚
â”‚  - Push metadata to Redisâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database / Data Lake    â”‚  â† Store metadata or final data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Core Components

### ğŸ•¸ `distributed-worker.py`

* Worker spider using `RedisSpider` to consume URLs from `url_pools:start_urls`.
* Loads domain-specific rules from `configs/DOM_site.json`.
* Extracts and cleans HTML using CSS selectors.
* Passes output to pipelines for saving.

---

### ğŸ“¦ `items.py`

Defines `ProductItem` model with fields:

```python
domain, url_item, status_code, source_page_html
```

Optionally prettified with **BeautifulSoup** for clean formatting.

---

### âš™ï¸ `pipelines.py`

Handles post-crawl actions:

* Save HTML to storage path.
* Push metadata (URL, domain, file path, status, timestamp) to Redis queue `scrapy:metadata`.
* Organize storage by domain.
* Use SHA256 for unique file naming.

---

### âš™ï¸ `settings.py`

Core Scrapy configuration:

* Enables distributed scheduler (`scrapy_redis.scheduler`).
* Custom downloader middlewares for proxy and UA rotation.
* Controls concurrency, delays, and pipeline priorities.

---

### ğŸ”§ `dataflow/load/add_url_to_pool.py`

Utility script to add new crawl URLs into Redis queue:

```bash
python -m vendor_scraper.dataflow.load.add_url_to_pool "https://example.com/page"
```

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/Dtsayy/scraper-website-project.git
cd cls-scraper-project
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -e .
# or, for dev mode
pip install -e .[dev]
```

### 3ï¸âƒ£ Configure Environment

Create a `.env` file in the root directory:

```env
REDIS_URL=redis://localhost:6379
POSTGRES_URL=postgresql://user:password@localhost:5432/vendor_db
```

### 4ï¸âƒ£ Verify Installation

```bash
scrapy list
# â†’ should show: distributed-worker
```

---

## ğŸ”„ Usage

### â¤ Add URLs to Queue

```bash
python -m vendor_scraper.dataflow.load.add_url_to_pool "https://example.com"
```

### â¤ Start a Distributed Worker

Each worker pulls from Redis automatically:

```bash
scrapy runspider vendor_scraper/spiders/distributed-worker.py
```

### â¤ Monitor Queue

```bash
redis-cli llen url_pools:start_urls
redis-cli llen scrapy:metadata
```

---

## ğŸ§¹ Output Example

**Stored HTML Path**

```text
html_storage/example.com/91e0b1...8f1b.html
```

**Metadata (Redis)**

```json
{
  "url": "https://example.com/page1",
  "domain": "example.com",
  "file_path": "./html_storage/example.com/91e0b1.html",
  "http_status": 200,
  "saved_date": "2025-10-25 12:10:33"
}
```

---

## ğŸ§© Future Improvements

* [ ] Add PostgreSQL loader pipeline
* [ ] Add retry + proxy rotation middleware
* [ ] Build dashboard for Redis queue monitoring
* [ ] Integrate Prefect / Airflow orchestration
* [ ] Automatic sitemap crawling

---

## ğŸ‘¨â€ğŸ’» Authors

**ThanhCD & TienTTM**
*Data Technician | Data Engineer Pathway*

---
